---
title: Elf-Disclosure / Apr 2025
description: April was "move-slow-and-fix-stuff", after the disruption of the holiday period, with a focus on expandability and setting ourselves up for more growth. Here some geeky stats, a retrospective, and a look ahead for ElfHosted from Apr 2025
---

discord: 2666


# "Elf-Disclosure" for Apr 2025

During March, we improved our cloud provider coverage by massively improving support for Premiumize and EasyNews (*with Debridav*), diversified our downloaders by adding a 3rd download-symlink tool, and added a few more apps to the catalogue.

To get us started, here are some geeky stats for Mar 2025, followed by a summary of some of the user-facing changes announced this month in the [blog](https://store.elfhosted.com/blog/)...

## Stats


=== ":seedling: Growth"

    :material-target: Focus            | :material-calendar: Feb 2025 | :material-calendar: Mar 2025 | :material-calendar: Apr 2025
    -----------------------------------|------------------------------|------------------------------|-----------------------------
    :simple-discord: [Discord][discord] members | 2495 | 2572 | 2666
    :simple-youtube: [YouTube](https://www.youtube.com/@elfhostme) subscribers | 678 | 694 | 735
    :simple-tiktok: [TikTok](https://www.tiktok.com/@elfhostme) followers | 28 | 27 | 27
    :simple-x: [X](https://x.com/elfhosted) followers | 96 | 98 | 102
    :simple-bluesky: [BlueSky](https://bsky.app/profile/elfhosted.com) followers  | - | 6 | 6
    :simple-mastodon: [Fediverse](https://mastodon.social/@elfhosted) followers  | - | 1 | 1


=== ":material-cpu-64-bit: CPU"

    The stats below illustrate CPU cores used (*not percentage*). These stats only cover the DE cluster at present, we're working on cross-cluster metrics aggregation to make this data more useful.

    Tenant CPU load on average is 20% higher than the preceding period (*again*), with the rest of the supporting infrastructure's CPU usage remaining relatively static.

    ![CPU stats for Apr 2025](/images/reports/2025-04/cluster-cpu-utilization.png)

    ??? "kubectl top nodes"
        ```

        ```

    Last month (*Mar*)'s for comparison:

    ![CPU stats for Mar 2025](/images/reports/2025-03/cluster-cpu-utilization.png)

    ??? "kubectl top nodes"
        ```
        NAME       CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)
        fairy01    657m         4%       25041Mi         19%
        fairy02    2033m        12%      54269Mi         42%
        fairy03    1788m        11%      40922Mi         31%
        gretel01   2971m        24%      28806Mi         44%
        gretel02   337m         2%       15937Mi         24%
        gretel03   145m         1%       9942Mi          15%
        gretel04   801m         6%       23121Mi         36%
        gretel07   360m         2%       21299Mi         16%
        gretel08   721m         4%       32298Mi         25%
        gretel09   459m         2%       21648Mi         16%
        gretel10   4242m        26%      58585Mi         45%
        gretel11   1210m        7%       32423Mi         25%
        gretel13   3241m        20%      30307Mi         23%
        gretel14   7668m        47%      40425Mi         31%
        gretel15   2799m        17%      29300Mi         22%
        gretel16   1859m        11%      22386Mi         17%
        gretel17   3755m        23%      32923Mi         25%
        gretel19   2537m        15%      80640Mi         62%
        gretel20   1937m        12%      28771Mi         22%
        gretel22   898m         5%       25351Mi         19%
        gretel23   2151m        13%      37010Mi         28%
        gretel26   2211m        13%      25469Mi         19%
        gretel27   2321m        14%      25358Mi         19%
        gretel30   987m         6%       23808Mi         37%
        gretel31   607m         3%       25442Mi         19%
        gretel33   2340m        14%      20301Mi         31%
        gretel37   3997m        24%      45488Mi         35%
        hansel01   1104m        9%       27231Mi         42%
        hansel02   2191m        18%      23370Mi         36%
        hansel04   713m         5%       27569Mi         42%
        hansel05   1446m        12%      17211Mi         26%
        hansel06   1651m        13%      23849Mi         37%
        hansel07   2341m        19%      25620Mi         39%
        hansel08   1369m        11%      22478Mi         35%
        hansel14   1909m        15%      27661Mi         43%
        hansel15   1249m        10%      43481Mi         67%
        hansel16   751m         6%       19163Mi         29%
        hansel17   705m         5%       19594Mi         30%
        hansel18   974m         8%       19538Mi         30%
        hansel20   1383m        11%      25043Mi         39%
        ```

=== ":material-memory: RAM"

    This graph represents memory usage across the entire (DE) cluster. One mildly interesting observation is that Feb's snapshot was taken when we were running Traefik v2, and during Mar we upgrade to to Traefik v3, and optimized our configs, such that overall RAM usage by Traefik is significantly lower.

    Other high consumers of RAM:

    * **csi-rclone**: used for mounting all rclone-compatible storage mounts, primarily RealDebrid libraries
    * **kube-system**: the Kubernetes control plane, including the cilium agents which manage the networking / policy enforcement (*currently 11K flows/s across 30 nodes*)
    * **traefik**: all inbound access to the cluster / services
    * **[mediafusion][mediafusion]**: an excellent (*but RAM-hungry!*) [Stremio addon](/stremio-addons/)

    ![Memory stats for Mar 2025](/images/reports/2025-03/cluster-memory-utilization.png)

    ??? "kubectl top nodes"
        ```

        ```

    Last month (*Feb 2025*)'s for comparison:

    ![Memory stats for Feb 2025](/images/reports/2025-02/cluster-memory-utilization.png)

    ??? "kubectl top nodes"
        ```
        NAME       CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)
        fairy01    657m         4%       25041Mi         19%
        fairy02    2033m        12%      54269Mi         42%
        fairy03    1788m        11%      40922Mi         31%
        gretel01   2971m        24%      28806Mi         44%
        gretel02   337m         2%       15937Mi         24%
        gretel03   145m         1%       9942Mi          15%
        gretel04   801m         6%       23121Mi         36%
        gretel07   360m         2%       21299Mi         16%
        gretel08   721m         4%       32298Mi         25%
        gretel09   459m         2%       21648Mi         16%
        gretel10   4242m        26%      58585Mi         45%
        gretel11   1210m        7%       32423Mi         25%
        gretel13   3241m        20%      30307Mi         23%
        gretel14   7668m        47%      40425Mi         31%
        gretel15   2799m        17%      29300Mi         22%
        gretel16   1859m        11%      22386Mi         17%
        gretel17   3755m        23%      32923Mi         25%
        gretel19   2537m        15%      80640Mi         62%
        gretel20   1937m        12%      28771Mi         22%
        gretel22   898m         5%       25351Mi         19%
        gretel23   2151m        13%      37010Mi         28%
        gretel26   2211m        13%      25469Mi         19%
        gretel27   2321m        14%      25358Mi         19%
        gretel30   987m         6%       23808Mi         37%
        gretel31   607m         3%       25442Mi         19%
        gretel33   2340m        14%      20301Mi         31%
        gretel37   3997m        24%      45488Mi         35%
        hansel01   1104m        9%       27231Mi         42%
        hansel02   2191m        18%      23370Mi         36%
        hansel04   713m         5%       27569Mi         42%
        hansel05   1446m        12%      17211Mi         26%
        hansel06   1651m        13%      23849Mi         37%
        hansel07   2341m        19%      25620Mi         39%
        hansel08   1369m        11%      22478Mi         35%
        hansel14   1909m        15%      27661Mi         43%
        hansel15   1249m        10%      43481Mi         67%
        hansel16   751m         6%       19163Mi         29%
        hansel17   705m         5%       19594Mi         30%
        hansel18   974m         8%       19538Mi         30%
        hansel20   1383m        11%      25043Mi         39%
        ```

=== ":material-server-network: Network"

    Network usage remains at about 20% of capacity on average, but (*due to thee changing nature of user behaviour*) this month there are more periods during which the effect of our egress rate-limiting can be clearly observed.
    
    !!! tip "Why Hansel & Gretel?"
        Bundles are datacenter-agnostic, but nodes are specific to each datacenter, and we needed a way to differentiate US nodes from DE nodes. The fairy tale of [Hansel and Gretel](https://en.wikipedia.org/wiki/Hansel_and_Gretel) originates in Germany :flag_de:

    ![Network traffic for Mar 2025 (*hansels*)](/images/reports/2025-03/cluster-network-utilization-hansels.png)
    
    ![Network traffic for Mar 2025 (*gretels)](/images/reports/2025-03/cluster-network-utilization-gretels.png)

    Last month (*Feb 2025*)'s for comparison:

    ![Network traffic for Feb 2025 (*hansels*)](/images/reports/2025-02/cluster-network-utilization-hansels.png)

    ![Network traffic for Feb 2025 (*gretels)](/images/reports/2025-02/cluster-network-utilization-gretels.png)

## Retrospective

### Decypharr

### Postgresql Aars

### Shim

### Huntarr

### Byparr

### SeerrBridge TV shows

### Plex remote streaming restrictions

Plex [announced significant changes](https://www.plex.tv/blog/important-2025-plex-updates/) to remote streaming from 29 April 2025. In effect, unless you're a Plex Pass subscriber, or you purchase a "Remote Watch Pass" subscription from Plex, you will no longer be able to stream your media "remotely" (*outside of the location where your Plex server is running*). While we're not sure yet exactly what the implications will be for our non-PlexPass users, the takeaway understood currently is this:

1. If you're a PlexPass user, nothing changes. Go back to bed :bed:
2. If you're **not** a PlexPass user but you **are** committed to the Plex ecosystem, you'd be advised to grab a lifetime [PlexPass membership](https://www.plex.tv/plex-pass/) during April, after which the price is set to more-than-double from USD $120 to $250.
3. If you're fed up with Plex and this is the last straw, you could consider your options by adding a [standalone Jellyfin](https://store.elfhosted.com/product/jellyfin-standalone/) or [standalone Emby](https://store.elfhosted.com/product/emby/) to your stack, evaluating them for the 7-day trial period, and then "switching" your stack to the Jellyfin/Emby equivalent once you've tested in parallel to your existing Plex setup. 
   At least that way you'll be well-informed when the hammer drops, and you discover that you've either got to pay Plex to continue streaming, or jump ship!

## Coming up


### US East Coast DC

The extremely handsome and intelligent Crunchbits chief-cable-monkey :monkey: Eric has confirmed that our US East Coast infrastructure is being deployed during the month of April. Quoting Eric:

> ElfHosted and their devishly charming leader @funkypenguin, are without a doubt our number-one priority for the entire PA datacenter build, and I will personally be working night and day to ensure that the East Coast Elfies get to enjoy the enhanced streaming speeds which await them upon completion of the PA rack build - 1 April 2025

### Symlink Cleaner

Our very own ElfVenger, @BSM, has been working on a new, Elf-sclusive app, cleverly named "Symlink Cleaner". Symlink Cleaner is currently available free as an [early-access trial](https://store.elfhosted.com/product/symlink-cleaner-early-access/), after which time we intend to roll it out to all Elfies, free of charge.

![](/images/screenshots/symlink-cleaner.png)

### AudioBookBayAutomated

[AudioBookBay Automated](https://store.elfhosted.com/product/audiobookbay-automated/) is another [bleeding-edge](https://github.com/JamesRy96/audiobookbay-automated) app which needs testing during March, but if you're an audiobook-phile, it may tweak your interest...

![](/images/screenshots/audiobookbayautomated.png)

### Even mooar apps

Apps currently requested can be found (and submitted!) [here](https://github.com/elfhosted/enhancements/issues?q=sort%3Aupdated-desc+is%3Aissue+is%3Aopen)

Notable suggestions:

* [TitleCardMaker](https://titlecardmaker.com/) (*CLI version for now*)
* [Profilarr](https://github.com/elfhosted/enhancements/issues/32): A UI-based tool to manage quality profiles in the Aaars

### ElfGuides (ongoing)

We've made videos about how to drive our most popular setups, but given the tools and apps change so fast, the videos very often become out-of-date. Re-recording a video simply to address a change a single tool in a larger workflow can be tedious and time-consuming, so we've been exploring another option.

The "ElfGuides" are a collection of ScribeHow documents, assembled modularly from a collection of "Scribes" (*screenshot-driven guides*), which can be mix/matched up to provide a detailed guide per-stack (*there are more than 30 variations now*!). When a tool in the stack changes, updating the guides is just a matter of updating the individual "module" covering that tool.

If you've been a long-time Elfie, you'll not have seen any guides, but they're emailed to new subscribers as they start their subscription!

The most popular app stacks are covered in the ElfGuides currently, but given the variety / rate of change we face, the effort to maintain these is... ongoing.

### Your ideas?

Got ideas for improvements? Send us an EEP (*ElfHosted Enhancement Proposal*) [here](https://github.com/elfhosted/enhancements/issues/new/choose)!

## How to help

Another effective way to help is to drive traffic / organic discovery, by mentioning ElfHosted in appropriate forums such as Reddit's [r/plex](https://reddit.com/r/plex),  [r/realdebrid](https://reddit.com/r/realdebrid), and [r/StremioAddons](https://reddit.com/r/StremioAddons), which is where much of our target audience is to be found!

## Join us!

!!! tip "Want to get involved?"

    Want to get involved? Join us in [Discord][discord]!

--8<-- "common-links.md"

{% include 'testimonials.md' %}