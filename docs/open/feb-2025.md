---
title: Elf-Disclosure / Feb 2025
description: January was "move-slow-and-fix-stuff", after the disruption of the holiday period, with a focus on expandability and setting ourselves up for more growth. Here some geeky stats, a retrospective, and a look ahead for ElfHosted from Jan 2025
---

# "Elf-Disclosure" for Feb 2025

During February, we added a few "fediverse" apps, a promising new download manager, hit an annoying storage upgrade bug, and matured both the US West Coast DC, and the Elf-illiate program.

To get us started, here are some geeky stats for Feb 2025, followed by a summary of some of the user-facing changes announced this month in the [blog](https://store.elfhosted.com/blog/)...

## Stats

=== ":seedling: Growth"

    :material-target: Focus            | :material-calendar: Dec 2024 | :material-calendar: Jan 2024 | :material-calendar: Feb 2025
    -----------------------------------|------------------------------|------------------------------|-----------------------------
    :simple-discord: [Discord][discord] members | 2328 | 2427 | 2495
    :simple-youtube: [YouTube](https://www.youtube.com/@elfhostme) subscribers | 618 | 654 | 678
    :simple-tiktok: [TikTok](https://www.tiktok.com/@elfhostme) followers | 28 | 28 | 28
    :simple-x: [X](https://x.com/elfhosted) followers  | 90 | 93 | 96
    :simple-bluesky: [BlueSky](https://bsky.app/profile/elfhosted.com) followers  | - | - | 1
    :simple-mastodon: [Fediverse](https://mastodon.social/@elfhosted) followers  | - | - | 0


=== ":material-cpu-64-bit: CPU"

    The stats below illustrate CPU cores used (*not percentage*). These stats only cover the DE cluster at present, we're working on cross-cluster metrics aggregation to make this data more useful.

    Tenant CPU load on average is lower than the previous month, but this may be because we encouraged more US users to migrate to the US datacenter in the wake of Hetzner speed/peering issues.

    ![CPU stats for Feb 2025](/images/reports/2025-02/cluster-cpu-utilization.png)

    ??? "kubectl top nodes"
        ```
        NAME       CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)
        fairy01    787m         4%       34155Mi         26%
        fairy02    1092m        6%       39519Mi         30%
        fairy03    1187m        7%       24686Mi         19%
        gretel01   3650m        30%      38618Mi         60%
        gretel02   511m         4%       14237Mi         22%
        gretel03   620m         5%       15344Mi         23%
        gretel07   632m         3%       31610Mi         24%
        gretel08   2179m        13%      30701Mi         23%
        gretel09   748m         4%       30012Mi         23%
        gretel10   2260m        14%      61511Mi         47%
        gretel11   1001m        6%       39465Mi         30%
        gretel13   592m         3%       27044Mi         21%
        gretel14   663m         4%       26131Mi         20%
        gretel15   1900m        11%      47002Mi         36%
        gretel16   1050m        6%       25330Mi         19%
        gretel17   1339m        8%       40058Mi         31%
        gretel19   519m         3%       13563Mi         10%
        gretel20   942m         5%       29987Mi         23%
        gretel22   1104m        6%       28272Mi         21%
        gretel23   1425m        8%       39173Mi         30%
        gretel26   963m         6%       24528Mi         19%
        gretel27   459m         2%       23879Mi         18%
        gretel30   2706m        16%      40510Mi         63%
        gretel31   5046m        31%      56759Mi         44%
        gretel33   747m         4%       16327Mi         25%
        gretel37   906m         5%       25094Mi         19%
        hansel01   1186m        9%       25611Mi         39%
        hansel02   1808m        15%      25012Mi         38%
        hansel04   2060m        17%      27165Mi         42%
        hansel05   2540m        21%      24557Mi         38%
        hansel06   1910m        15%      21074Mi         32%
        hansel07   2718m        22%      28893Mi         45%
        hansel08   927m         7%       32238Mi         50%
        hansel14   1295m        10%      29500Mi         45%
        hansel15   1960m        16%      42793Mi         66%
        hansel16   1385m        11%      29855Mi         46%
        hansel17   1273m        10%      21838Mi         34%
        hansel18   2091m        17%      30429Mi         47%
        hansel20   1544m        12%      26133Mi         40%
        ```

    Last month (*Dec*)'s for comparison:

    ![CPU stats for Jan 2025](/images/reports/2025-01/cluster-cpu-utilization.png)

    ??? "kubectl top nodes"
        ```
        NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
        fairy01    3253m        20%    48989Mi         38%
        fairy02    2219m        13%    38351Mi         29%
        fairy03    3600m        22%    35117Mi         27%
        gretel01   1976m        16%    31328Mi         48%
        gretel02   882m         7%     20904Mi         32%
        gretel07   1617m        10%    27626Mi         21%
        gretel08   2241m        14%    41597Mi         32%
        gretel09   1145m        7%     28556Mi         22%
        gretel10   1587m        9%     27677Mi         21%
        gretel11   5257m        32%    40538Mi         31%
        gretel13   681m         4%     64061Mi         49%
        gretel14   944m         5%     26839Mi         20%
        gretel15   1624m        10%    35381Mi         27%
        gretel16   1876m        11%    29543Mi         22%
        gretel17   2683m        16%    40151Mi         31%
        gretel19   1813m        11%    25495Mi         19%
        gretel20   1739m        10%    39069Mi         30%
        gretel22   756m         4%     20475Mi         15%
        gretel23   791m         4%     28535Mi         22%
        gretel26   1452m        9%     24623Mi         19%
        gretel27   1378m        8%     19278Mi         14%
        gretel30   6652m        41%    31794Mi         49%
        gretel31   2038m        12%    82366Mi         63%
        gretel33   2047m        12%    30128Mi         46%
        gretel37   3329m        20%    27933Mi         21%
        hansel01   3332m        27%    49937Mi         77%
        hansel02   2432m        20%    43139Mi         67%
        hansel03   1523m        19%    32627Mi         50%
        hansel13   2077m        25%    45012Mi         70%
        hansel14   3078m        25%    43553Mi         67%
        hansel15   1652m        13%    46544Mi         72%
        hansel16   2741m        22%    42101Mi         65%
        hansel17   1620m        13%    30437Mi         47%
        hansel18   2203m        18%    40614Mi         63%
        hansel19   6146m        51%    48192Mi         75%
        hansel20   1712m        14%    25911Mi         40%
        ```

=== ":material-memory: RAM"

    This graph represents memory usage across the entire (DE) cluster.

    Other high consumers of RAM:

    * **csi-rclone**: used for mounting all rclone-compatible storage mounts, primarily RealDebrid libraries
    * **kube-system**: the Kubernetes control plane, including the cilium agents which manage the networking / policy enforcement (*currently 11K flows/s across 30 nodes*)
    * **traefik**: all inbound access to the cluster / services
    * **[mediafusion][mediafusion]**: an excellent (*but RAM-hungry!*) [Stremio addon](/stremio-addons/)

    ![Memory stats for Feb 2025](/images/reports/2025-02/cluster-memory-utilization.png)

    ??? "kubectl top nodes"
        ```
        NAME       CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)
        fairy01    787m         4%       34155Mi         26%
        fairy02    1092m        6%       39519Mi         30%
        fairy03    1187m        7%       24686Mi         19%
        gretel01   3650m        30%      38618Mi         60%
        gretel02   511m         4%       14237Mi         22%
        gretel03   620m         5%       15344Mi         23%
        gretel07   632m         3%       31610Mi         24%
        gretel08   2179m        13%      30701Mi         23%
        gretel09   748m         4%       30012Mi         23%
        gretel10   2260m        14%      61511Mi         47%
        gretel11   1001m        6%       39465Mi         30%
        gretel13   592m         3%       27044Mi         21%
        gretel14   663m         4%       26131Mi         20%
        gretel15   1900m        11%      47002Mi         36%
        gretel16   1050m        6%       25330Mi         19%
        gretel17   1339m        8%       40058Mi         31%
        gretel19   519m         3%       13563Mi         10%
        gretel20   942m         5%       29987Mi         23%
        gretel22   1104m        6%       28272Mi         21%
        gretel23   1425m        8%       39173Mi         30%
        gretel26   963m         6%       24528Mi         19%
        gretel27   459m         2%       23879Mi         18%
        gretel30   2706m        16%      40510Mi         63%
        gretel31   5046m        31%      56759Mi         44%
        gretel33   747m         4%       16327Mi         25%
        gretel37   906m         5%       25094Mi         19%
        hansel01   1186m        9%       25611Mi         39%
        hansel02   1808m        15%      25012Mi         38%
        hansel04   2060m        17%      27165Mi         42%
        hansel05   2540m        21%      24557Mi         38%
        hansel06   1910m        15%      21074Mi         32%
        hansel07   2718m        22%      28893Mi         45%
        hansel08   927m         7%       32238Mi         50%
        hansel14   1295m        10%      29500Mi         45%
        hansel15   1960m        16%      42793Mi         66%
        hansel16   1385m        11%      29855Mi         46%
        hansel17   1273m        10%      21838Mi         34%
        hansel18   2091m        17%      30429Mi         47%
        hansel20   1544m        12%      26133Mi         40%
        ```

    Last month (*Jan 2025*)'s for comparison:

    ![Memory stats for Jan 2025](/images/reports/2025-01/cluster-memory-utilization.png)

    ??? "kubectl top nodes"
        ```
        NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
        fairy01    3253m        20%    48989Mi         38%
        fairy02    2219m        13%    38351Mi         29%
        fairy03    3600m        22%    35117Mi         27%
        gretel01   1976m        16%    31328Mi         48%
        gretel02   882m         7%     20904Mi         32%
        gretel07   1617m        10%    27626Mi         21%
        gretel08   2241m        14%    41597Mi         32%
        gretel09   1145m        7%     28556Mi         22%
        gretel10   1587m        9%     27677Mi         21%
        gretel11   5257m        32%    40538Mi         31%
        gretel13   681m         4%     64061Mi         49%
        gretel14   944m         5%     26839Mi         20%
        gretel15   1624m        10%    35381Mi         27%
        gretel16   1876m        11%    29543Mi         22%
        gretel17   2683m        16%    40151Mi         31%
        gretel19   1813m        11%    25495Mi         19%
        gretel20   1739m        10%    39069Mi         30%
        gretel22   756m         4%     20475Mi         15%
        gretel23   791m         4%     28535Mi         22%
        gretel26   1452m        9%     24623Mi         19%
        gretel27   1378m        8%     19278Mi         14%
        gretel30   6652m        41%    31794Mi         49%
        gretel31   2038m        12%    82366Mi         63%
        gretel33   2047m        12%    30128Mi         46%
        gretel37   3329m        20%    27933Mi         21%
        hansel01   3332m        27%    49937Mi         77%
        hansel02   2432m        20%    43139Mi         67%
        hansel03   1523m        19%    32627Mi         50%
        hansel13   2077m        25%    45012Mi         70%
        hansel14   3078m        25%    43553Mi         67%
        hansel15   1652m        13%    46544Mi         72%
        hansel16   2741m        22%    42101Mi         65%
        hansel17   1620m        13%    30437Mi         47%
        hansel18   2203m        18%    40614Mi         63%
        hansel19   6146m        51%    48192Mi         75%
        hansel20   1712m        14%    25911Mi         40%
        ```

=== ":material-server-network: Network"

    Last month's spikes on the contended nodes (*hansels*) turned out to be related to in-cluster backups, rather than tenant-driven load, and this misconfiguration was resolved. Hansel and Gretel traffic patterns are now more aligned to what you'd expect, comparing December to November:
    
    !!! tip "Why Hansel & Gretel?"
        Bundles are datacenter-agnostic, but nodes are specific to each datacenter, and we needed a way to differentiate US nodes from DE nodes. The fairy tale of [Hansel and Gretel](https://en.wikipedia.org/wiki/Hansel_and_Gretel) originates in Germany :flag_de:

    ![Network traffic for Feb 2025 (*hansels*)](/images/reports/2025-02/cluster-network-utilization-hansels.png)
    
    ![Network traffic for Feb 2025 (*gretels)](/images/reports/2025-02/cluster-network-utilization-gretels.png)

    Last month (*Jan 2025*)'s for comparison:

    ![Network traffic for Jan 2025 (*hansels*)](/images/reports/2025-01/cluster-network-utilization-hansels.png)

    ![Network traffic for Jan 2025 (*gretels)](/images/reports/2025-01/cluster-network-utilization-gretels.png)

## Retrospective

February saw the return of [Comet][comet] (v2.0), renewed interested in "fediverse" social apps ([GoToSocial][gotosocial] and [BlueSky PDS][bluesky-pds]), and the introduction of another suite of media-management tools ([SeerrBridge][seerrbridge], [ListSync][listsync])

### Storage bug bites

A [theoretically-harmless upgrade](https://github.com/elfhosted/infra/pull/1333) to our cluster's storage layer resulted in some users having their config volumes wiped, and we had a busy few days restoring from daily backups. 

The troublesome upgrade was [rolled back,](https://github.com/elfhosted/infra/pull/1534) and we've not yet re-attempted it (*works fine in CI of course :facepalm:*), but we'll probably take advantage of the US East Coast DC build (*see below*) to test it further, without risking further impact. 

There's a possibility that applying the upgrade safely will require scaling down of all workloads beforehand, which will be a significant undertaking during a weekday glowup, and will need careful co-ordination.

### Comet is back

After development initially petered out in Dec/Jan, Comet developer G0ldyy pushed hard in Feb and released v2.0 ("the rewrite"). 

In an indication of the growing maturity and interconnectedness of the Stremio Addon / Debrid ecosystem, Comet 2.0 relies on StremThru's crowdsourced database of cached hashes (*since these are no longer available from RD/AD/DL*), so the public (*and private!*) instances are used, the better the cache database becomes.

ElfHosted [Comet][comet] users continue to enjoy perks such as:

* No rate-limits
* TorrentIO scraping
* Zilean DMM scraping ([super-charged with Zyclops](https://store.elfhosted.com/blog/2024/11/19/project-zycops-is-a-go/))
* Proxy-streaming support (*64Mbps bundled, boostable*)
* 33% of your subscription goes to the developer

### Mooar apps

The following apps made their debut on ElfHosted during Feb 2025:

#### GoToSocial

Want to experiment with Mastodon, but not sure where to start?

[GoToSocial][gotosocial] is a lightweight, secure, and private ActivityPub social network server, your gateway to the larger Fediverse.

With GoToSocial, you can keep in touch with your friends, post, read, and share images and articles. All without being tracked or advertised to!

![Screenshot of GoToSocial](/images/screenshots/gotosocial.png){ loading=lazy }

More details [here][gotosocial], in [this blog post](https://store.elfhosted.com/blog/2025/02/17/join-the-fediverse-with-gotosocial-for-1-month/), and on [our Mastodon profile](https://mastodon.social/@elfhosted).

#### BlueSky PDS

Bluesky is an ambitious federated social network initially supported by Twitter, but is an independent public benefit corporation as of 2022 ([Wikipedia](https://en.wikipedia.org/wiki/Bluesky))

The network is federated with the ATProtocol, which allows all participating users to communicate through a series of relays, whether their account is "on" the primary bsky.social server, or on their own, independently-managed data server instance.

A Personal Data Server (PDS) is a small server which is the "home" for one or more accounts, and serves to manage all their data storage, "distributing" the data separately from the other components of the network (*relaying, scraping, etc*), and giving users control of the presentation and storage of their data.

![Screenshot of BlueSky PDS](/images/screenshots/bluesky-pds.png){ loading=lazy }

More details [here][bluesky-pds], in [this blog post](https://store.elfhosted.com/blog/2025/02/19/own-your-social-content-with-bluesky-pds/), and in our [BlueSky profile](https://bsky.app/profile/elfhosted.com).

#### SeerrBridge

[SeerrBridge] is a clever implementation which bypasses the "classic" way of building an infinite streaming library (plex_debrid, Aars, or Riven), and simply uses a headless browser to interact directly with DebridMediaManager, fulfilling requests from [Overseerr][overseerr] / [Jellyseerr][jellyseerr].

![Screenshot of SeerrBridge](/images/screenshots/seerrbridge.png){ loading=lazy }

It's notably a cutting-edge, still-developing tool, but it's a workable alternative to the more complex stacks, with the advantage of having full access to the entirety of the DebridMediaManager database - something no other download stack can do!

More details [here][seerrbridge], and integrated bundles are [available in the store](https://store.elfhosted.com/?s=seerrbridge&post_type=product&product_cat=0) - you could, for example, switch out a Riven stack for a SeerrBridge stack, with relatively very little friction.

#### ListSync

Many tools integrate with OverSeerr for the addition of content, but OverSeerr itself can't populate requests other than from users' Plex watchlists.

[ListSync][listsync] is a tool by the developer of [SeerrBridge][seerrbridge] (*you can see why they work, hand-in-hand*), which will create Overseerr requests from a collection of upstream lists from IMDB, Trakt, or Letterboxd.

Useful everywhere Overseerr is used, but specifically in a SeerrBridge-managed stack, the importing of upstream lists would fully automate the creation of a library based on a combination of popular lists, such as [Gary's top movies of the week](https://trakt.tv/users/garycrawfordgc/lists/top-movies-of-the-week?sort=rank,asc).

![Screenshot of ListSync](/images/screenshots/listsync.png){ loading=lazy }

More details [here][listsync]

### US West Coast DC

During February, there was a period during which Hetzner was experiencing peering / throughput issues, and several of our US-based users successfully migrated their stacks over to the US West Coast (Washington State) DC. 

A targeted email was sent out at the time to US users, but if you **didn't** receive an invitation, but the results from https://speed.elfhosted.com indicate that you'd be better off connected to the US, you could benefit from a [relocation](/how-to/migrate-datacenters/).

## Coming up

### US East Coast DC

Well. We have news. Here's where our PA DC will go:

![](/images/reports/2025-02/racks_close_up.png)

And here's a wider shot:

![](/images/reports/2025-02/racks_long_shot.png)

It's tentatively looking as if we'll get an installation in 2-3 weeks, so ideally, look forward to the East Coast DC being featured in next month's report! :fingers_crossed:

### Elf-illiate program matures

During January, we transitioned our (*previously manual and dumb*) developer contributions system to a more advanced "Elf-illiate" program, which allows us to calculate and pay out our participating open-source developers for a portion of subscription fees.

We expanded this rollout over February, finding a few bugs and configuration issues as we progressed.

Unlike the referral program, the [affiliate program](https://store.elfhosted.com/affiliate/) requires approval, but pays out directly in cash, as opposed to account credit. The idea is to encourage users with an "audience" (*elf-influencers?*) to spread the word about ElfHosted, in return for a percentage of commission on sales.

If you're interested in becoming "Elf-iliated", start your application [here](https://store.elfhosted.com/affiliate/).

### Even mooar apps

Apps currently requested can be found (and submitted!) [here](https://github.com/elfhosted/enhancements/issues?q=sort%3Aupdated-desc+is%3Aissue+is%3Aopen)

Notable suggestions:

* [TitleCardMaker](https://titlecardmaker.com/) (*CLI version for now*)
* [PlexRequests](https://github.com/westsurname/scripts/releases)

### ElfGuides (ongoing)

We've made videos about how to drive our most popular setups, but given the tools and apps change so fast, the videos very often become out-of-date. Re-recording a video simply to address a change a single tool in a larger workflow can be tedious and time-consuming, so we've been exploring another option.

The "ElfGuides" are a collection of ScribeHow documents, assembled modularly from a collection of "Scribes" (*screenshot-driven guides*), which can be mix/matched up to provide a detailed guide per-stack (*there are more than 30 variations now*!). When a tool in the stack changes, updating the guides is just a matter of updating the individual "module" covering that tool.

If you've been a long-time Elfie, you'll not have seen any guides, but they're emailed to new subscribers as they start their subscription!

The most popular app stacks are covered in the ElfGuides currently, but given the variety / rate of change we face, the effort to maintain these is... ongoing.

### Your ideas?

Got ideas for improvements? Send us an EEP (*ElfHosted Enhancement Proposal*) [here](https://github.com/elfhosted/enhancements/issues/new/choose)!

## How to help

Another effective way to help is to drive traffic / organic discovery, by mentioning ElfHosted in appropriate forums such as Reddit's [r/plex](https://reddit.com/r/plex),  [r/realdebrid](https://reddit.com/r/realdebrid), and [r/StremioAddons](https://reddit.com/r/StremioAddons), which is where much of our target audience is to be found!

## Join us!

!!! tip "Want to get involved?"

    Want to get involved? Join us in [Discord][discord]!

--8<-- "common-links.md"

{% include 'testimonials.md' %}